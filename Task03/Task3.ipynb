{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Read and Preprocess gold_prices.csv**"
      ],
      "metadata": {
        "id": "jVP6CIv1o6Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"GoldPricePrediction\").getOrCreate()\n",
        "\n",
        "# Load CSV from Google Drive or local\n",
        "csv_path = \"/content/drive/MyDrive/MMDS/Final/gold_prices.csv\"\n",
        "\n",
        "# Read CSV with headers and infer schema\n",
        "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(csv_path)"
      ],
      "metadata": {
        "id": "BRlkzZc2o5Qc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show schema and preview\n",
        "df.printSchema()\n",
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keYdK0i4pbHP",
        "outputId": "a5492d5b-f2ab-49d1-9bb9-2a770912779f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Buy Price: double (nullable = true)\n",
            " |-- Sell Price: double (nullable = true)\n",
            "\n",
            "+----------+---------+----------+\n",
            "|      Date|Buy Price|Sell Price|\n",
            "+----------+---------+----------+\n",
            "|2009-08-01|    21.13|     21.19|\n",
            "|2009-08-02|    21.13|     21.19|\n",
            "|2009-08-03|    21.13|     21.19|\n",
            "|2009-08-04|    21.13|     21.19|\n",
            "|2009-08-05|    21.13|     21.19|\n",
            "|2009-08-06|    21.13|     21.19|\n",
            "|2009-08-07|    21.13|     21.19|\n",
            "|2009-08-08|    21.13|     21.19|\n",
            "|2009-08-09|    21.13|     21.19|\n",
            "|2009-08-10|    21.13|     21.19|\n",
            "+----------+---------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
        "df = df.withColumn(\"Buy Price\", col(\"Buy Price\").cast(\"double\"))\n",
        "df = df.withColumn(\"Sell Price\", col(\"Sell Price\").cast(\"double\"))\n",
        "\n",
        "# Sort by date (important for lag features)\n",
        "df = df.orderBy(\"Date\")\n",
        "\n",
        "# Optional check\n",
        "print(\"Total rows:\", df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLltm6Bko-nc",
        "outputId": "5c47d7aa-c47d-4a59-8088-02a76822dedf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 5565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introduction**\n",
        "Objective: Reduce the 10-dimensional feature vectors generated in Task 2 to 5 dimensions using CUR decomposition.\n",
        "\n",
        "Goal: Improve model efficiency or generalization by reducing input dimensionality, while maintaining comparable predictive performance."
      ],
      "metadata": {
        "id": "VxX5SQdEiVII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Reuse Samples from Task 2**\n",
        "Use the feature-label dataset already created in Task 2:\n",
        "\n",
        "Features: Buy prices of the previous 10 days.\n",
        "\n",
        "Label: Buy price on the current day.\n",
        "\n",
        "Use the same 70/30 train/test split for fair comparison."
      ],
      "metadata": {
        "id": "JfH3f-Ydk2Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import lag\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# 1. Create lag features for the previous 10 days\n",
        "windowSpec = Window.orderBy(\"Date\")\n",
        "\n",
        "for i in range(1, 11):\n",
        "    df = df.withColumn(f\"lag_{i}\", lag(\"Buy Price\", i).over(windowSpec))\n",
        "\n",
        "# 2. Drop any rows with null values introduced by lag\n",
        "df = df.na.drop()\n",
        "\n",
        "# 3. Create feature columns list\n",
        "feature_cols = [f\"lag_{i}\" for i in range(10, 0, -1)]  # From lag_10 to lag_1\n",
        "\n",
        "# 4. Assemble features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# 5. Final dataset with label and features\n",
        "data = df.select(\"features\", col(\"Buy Price\").alias(\"label\"))\n",
        "\n",
        "# 6. Split into training and test sets (70/30)\n",
        "train_df, test_df = data.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# 7. Preview\n",
        "train_df.show(3, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIpEvFjbmxlG",
        "outputId": "0146a08a-d619-40f8-e6e0-478a0d89aa47"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------+-----+\n",
            "|features                                                     |label|\n",
            "+-------------------------------------------------------------+-----+\n",
            "|[21.13,21.13,21.13,21.13,21.13,21.13,21.13,21.13,21.13,21.13]|21.13|\n",
            "|[21.13,21.13,21.13,21.13,21.13,21.13,21.13,21.13,21.13,21.13]|21.13|\n",
            "|[21.13,21.13,21.13,21.13,21.13,21.13,21.13,21.13,21.13,21.13]|21.13|\n",
            "+-------------------------------------------------------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Apply CUR Decomposition to Reduce Dimensionality**\n",
        "**3.1 Convert Training and Test Features to RowMatrix**\n",
        "\n",
        "Convert both training and test sets into PySpark RowMatrix format to enable distributed matrix operations.\n",
        "\n",
        "**3.2 Compute SVD on Training Data**\n",
        "\n",
        "Perform truncated Singular Value Decomposition (SVD) on the training matrix to obtain U, S, and Vᵗ.\n",
        "\n",
        "**3.3 Select Columns and Rows Using Leverage Scores**\n",
        "\n",
        "Compute leverage scores:\n",
        "\n",
        "From Vᵗ → select 5 informative columns.\n",
        "\n",
        "From U → select 5 representative rows.\n",
        "\n",
        "Select indices based on sampling from these scores.\n",
        "\n",
        "**3.4 Construct CUR Decomposition**\n",
        "\n",
        "Extract submatrices:\n",
        "\n",
        "C: columns from original matrix.\n",
        "\n",
        "R: rows from original matrix.\n",
        "\n",
        "W: intersection matrix of selected rows and columns.\n",
        "\n",
        "Compute pseudo-inverse of W.\n",
        "\n",
        "Use CUR formula to reconstruct approximate matrix: A ≈ C × W⁺ × R\n",
        "\n",
        "**3.5 Generate 5D Embeddings**\n",
        "\n",
        "Use the CUR approximation to project the original data into a new 5-dimensional representation (row embeddings).\n",
        "\n",
        "Apply this process to both training and test sets."
      ],
      "metadata": {
        "id": "s3O3nPsLk8iW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 – Convert Training and Test Features to RowMatrix\n",
        "from pyspark.mllib.linalg.distributed import RowMatrix\n",
        "\n",
        "train_rdd = train_df.select(\"features\").rdd.map(lambda row: row.features.toArray())\n",
        "test_rdd = test_df.select(\"features\").rdd.map(lambda row: row.features.toArray())\n",
        "\n",
        "train_matrix = RowMatrix(train_rdd)\n",
        "test_matrix = RowMatrix(test_rdd)\n",
        "\n",
        "print(\"Training matrix shape:\", train_matrix.numRows(), \"x\", train_matrix.numCols())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a4NK1M3q-1A",
        "outputId": "7d92184d-df9e-4e23-e008-cd07806d9a0b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training matrix shape: 3964 x 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2 – Compute Truncated SVD on Training Data\n",
        "k = 5\n",
        "num_cols = train_matrix.numCols()\n",
        "num_rows = train_matrix.numRows()\n",
        "\n",
        "# Ensure k is safe\n",
        "k = min(k, num_cols, num_rows - 1)\n",
        "\n",
        "svd = train_matrix.computeSVD(k, computeU=True)\n",
        "U = svd.U  # RowMatrix\n",
        "S = svd.s  # Vector\n",
        "V = svd.V  # Local Matrix (k x numCols)\n",
        "\n",
        "print(\"SVD done. V shape:\", V.numRows, \"x\", V.numCols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NFse-AdGbO1",
        "outputId": "c79d00c4-c4ec-42b9-db68-6056eebb4e76"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVD done. V shape: 10 x 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.3 – Select Columns and Rows Using Leverage Scores\n",
        "import numpy as np\n",
        "\n",
        "# Column leverage scores from Vᵗ\n",
        "V_array = V.toArray()  # shape: k x numCols\n",
        "V_T = V_array.T        # shape: numCols x k\n",
        "col_scores = np.sum(V_T ** 2, axis=1)\n",
        "col_probs = col_scores / np.sum(col_scores)\n",
        "col_indices = np.random.choice(len(col_probs), size=k, replace=False, p=col_probs)\n",
        "col_indices.sort()\n",
        "\n",
        "# Row leverage scores from U\n",
        "U_array = np.array(U.rows.map(lambda v: np.array(v)).collect())  # small U matrix, safe to collect\n",
        "row_scores = np.sum(U_array ** 2, axis=1)\n",
        "row_probs = row_scores / np.sum(row_scores)\n",
        "row_indices = np.random.choice(len(row_probs), size=k, replace=False, p=row_probs)\n",
        "row_indices.sort()"
      ],
      "metadata": {
        "id": "Th54oTCJG_8e"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.4 – Construct CUR Decomposition\n",
        "# Full training matrix as local array (only once)\n",
        "A_train = np.array(train_matrix.rows.map(lambda v: np.array(v)).collect())\n",
        "\n",
        "# Extract C (all rows, selected columns), R (selected rows, all columns), W (intersection)\n",
        "C = A_train[:, col_indices]           # shape: n x k\n",
        "R = A_train[row_indices, :]           # shape: k x d\n",
        "W = A_train[np.ix_(row_indices, col_indices)]  # shape: k x k\n",
        "\n",
        "# Compute pseudo-inverse of W\n",
        "W_pinv = np.linalg.pinv(W)"
      ],
      "metadata": {
        "id": "aKLF_nrtGfag"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.5 – Generate CUR Embeddings for Train and Test Sets\n",
        "# CUR projection: A' = C × W⁺ × R\n",
        "CUR_train = np.dot(np.dot(C, W_pinv), R)  # shape: n x d\n",
        "CUR_train_reduced = CUR_train[:, :k]      # shape: n x 5"
      ],
      "metadata": {
        "id": "Vy5vrhxSGlgi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply same column indices and projection logic to test matrix\n",
        "A_test = np.array(test_matrix.rows.map(lambda v: np.array(v)).collect())\n",
        "C_test = A_test[:, col_indices]\n",
        "CUR_test = np.dot(np.dot(C_test, W_pinv), R)\n",
        "CUR_test_reduced = CUR_test[:, :k]        # shape: n x 5"
      ],
      "metadata": {
        "id": "qVtqkk40HK0U"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Train and Evaluate a New Linear Regression Model**\n",
        "Use the CUR-reduced training set to train a PySpark linear regression model.\n",
        "\n",
        "Predict on both CUR training and test sets."
      ],
      "metadata": {
        "id": "QWkND3pzk_uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 Convert CUR Embeddings to PySpark DataFrames\n",
        "from pyspark.ml.linalg import Vectors as MLVectors\n",
        "\n",
        "# Extract labels from original DataFrames\n",
        "train_labels = [float(row[\"label\"]) for row in train_df.select(\"label\").collect()]\n",
        "test_labels = [float(row[\"label\"]) for row in test_df.select(\"label\").collect()]\n",
        "\n",
        "# Pair features and labels for training set\n",
        "train_data = [(MLVectors.dense(vec), label) for vec, label in zip(CUR_train_reduced, train_labels)]\n",
        "test_data = [(MLVectors.dense(vec), label) for vec, label in zip(CUR_test_reduced, test_labels)]\n",
        "\n",
        "# Create Spark DataFrames\n",
        "train_df_cur = spark.createDataFrame(train_data, [\"features\", \"label\"])\n",
        "test_df_cur = spark.createDataFrame(test_data, [\"features\", \"label\"])\n",
        "\n",
        "train_df_cur.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1VSJ5plmyb4",
        "outputId": "f81bb6f0-6c89-499c-b0ed-2144014c9e6c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------+-----+\n",
            "|features                                                                                      |label|\n",
            "+----------------------------------------------------------------------------------------------+-----+\n",
            "|[21.129999999998777,21.129999999998812,21.129999999998844,21.12999999999891,21.12999999999892]|21.13|\n",
            "|[21.129999999998777,21.129999999998812,21.129999999998844,21.12999999999891,21.12999999999892]|21.13|\n",
            "|[21.129999999998777,21.129999999998812,21.129999999998844,21.12999999999891,21.12999999999892]|21.13|\n",
            "|[21.129999999998777,21.129999999998812,21.129999999998844,21.12999999999891,21.12999999999892]|21.13|\n",
            "|[21.129999999998777,21.129999999998812,21.129999999998844,21.12999999999891,21.12999999999892]|21.13|\n",
            "+----------------------------------------------------------------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2 Train PySpark Linear Regression Model on CUR-Reduced Data\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "lr_model_cur = lr.fit(train_df_cur)\n",
        "\n",
        "# Predict on train and test sets\n",
        "train_pred_cur = lr_model_cur.transform(train_df_cur)\n",
        "test_pred_cur = lr_model_cur.transform(test_df_cur)"
      ],
      "metadata": {
        "id": "HxZHi8pdHaZD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.3 Evaluate with MSE, RMSE, MAE\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_mse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "evaluator_mae = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "\n",
        "# Train set metrics\n",
        "rmse_train_cur = evaluator_rmse.evaluate(train_pred_cur)\n",
        "mse_train_cur = evaluator_mse.evaluate(train_pred_cur)\n",
        "mae_train_cur = evaluator_mae.evaluate(train_pred_cur)\n",
        "\n",
        "# Test set metrics\n",
        "rmse_test_cur = evaluator_rmse.evaluate(test_pred_cur)\n",
        "mse_test_cur = evaluator_mse.evaluate(test_pred_cur)\n",
        "mae_test_cur = evaluator_mae.evaluate(test_pred_cur)\n",
        "\n",
        "print(\"=== CUR Regression Metrics ===\")\n",
        "print(f\"Train RMSE: {rmse_train_cur:.4f}, MSE: {mse_train_cur:.4f}, MAE: {mae_train_cur:.4f}\")\n",
        "print(f\"Test  RMSE: {rmse_test_cur:.4f}, MSE: {mse_test_cur:.4f}, MAE: {mae_test_cur:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0-vdCDvHdrE",
        "outputId": "34d796d8-505e-43e5-aa3c-22490c85d26b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CUR Regression Metrics ===\n",
            "Train RMSE: 0.6927, MSE: 0.4799, MAE: 0.3832\n",
            "Test  RMSE: 0.9438, MSE: 0.8908, MAE: 0.4473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Evaluate and Compare Model Performance**\n",
        "CUR-reduced (Task 3) models using:\n",
        "\n",
        "MSE – Mean Squared Error\n",
        "\n",
        "RMSE – Root Mean Squared Error\n",
        "\n",
        "MAE – Mean Absolute Error"
      ],
      "metadata": {
        "id": "emM8Y9uSlBDE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yIGyOWaPmywi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Visualize the Results**\n",
        "Bar Chart: Compare RMSEs between:\n",
        "\n",
        "Original training\n",
        "\n",
        "Original test\n",
        "\n",
        "CUR training\n",
        "\n",
        "CUR test\n",
        "\n",
        "(Optional) Grouped Bar Chart: Visualize all three metrics (MSE, RMSE, MAE) for both models across both datasets."
      ],
      "metadata": {
        "id": "nfvIZVgBlCbI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W-cWajL6mzFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Analysis and Interpretation**\n",
        "Discuss whether dimensionality reduction preserved predictive accuracy.\n",
        "\n",
        "Analyze performance trade-offs (accuracy vs. dimension).\n",
        "\n",
        "Comment on whether CUR helped reduce overfitting or improved computational efficiency."
      ],
      "metadata": {
        "id": "j7Fm62tTlDzd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8JCoQczpmzbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Conclusion**\n",
        "Summarize benefits of CUR.\n",
        "\n",
        "Discuss applicability for larger-scale time series or other real-world datasets.\n",
        "\n",
        "Suggest future enhancements (e.g., CUR + regularization, or CUR vs. PCA).\n",
        "\n"
      ],
      "metadata": {
        "id": "MCBtRL9Im-ck"
      }
    }
  ]
}